version: "3"

services:
  
  airflow-server:
    image: makengi12/airflow:1.5
    #image: airflow_test
    container_name: airflow-server
    # entrypoint: ./run.sh
    build: ./airflow
    volumes:
      - airflow:/root/airflow
    ports:
      - 8086:8086
    restart: "on-failure"
    stdin_open: true
    tty: true
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_COMMON_HOME=/opt/hadoop
      - HADOOP_MAPRED_HOME=/opt/hadoop

  namenode:
    image: makengi12/namenode:1.0
    # build: ./namenode
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - 9870:9870
        #      - 9010:9000
      - 8042:8042
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://0.0.0.0:9000
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    restart: always
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://192.168.0.6:9000
    ports:
      - "9864:9864"
      - 9866:9866
    env_file:
      - ./hadoop.env


  resourcemanager:
    image: makengi12/resourcemanager:1.0
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    ports:
      - "8088:8088"
      - "8032:8032"

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  spark-master:
    image: makengi12/spark-master:1.1
    container_name: spark-master
    # build: ./master
    depends_on:
      - namenode
      - datanode
    ports:
      - "8085:8085"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      

  spark-worker-1:
    image: makengi12/spark-worker:1.0
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - MEMORY=4g
      - CORE=3

  spark-worker-2:
    image: makengi12/spark-worker:1.0
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - MEMORY=4g
      - CORE=3  

  spark-worker-3:
    image: makengi12/spark-worker:1.0
    container_name: spark-worker-3
    depends_on:
      - spark-master
    ports:
      - "8083:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"    
      - MEMORY=4g
      - CORE=2


  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
      - "10002:10002"

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    ports:
      - "8089:8089"

  zeppelin:
    image: makengi12/zeppelin:1.0
    container_name: zeppelin
    # build: ./zeppelin
    ports:
      - "8080:8080"
    environment:
      ZEPPELIN_ADDR: '0.0.0.0'
      ZEPPELIN_NOTEBOOK_DIR: '/notebook'
      ZEPPELIN_LOG_DIR: '/logs'
      SPARK_HOME: '/spark'
    volumes:
       - zeppelin_logs:/logs
       - zeppelin_data:/data
       - zeppelin_notebook:/notebook 

  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
       - grafana:/var/lib/grafana


  hbase:
    image: makengi12/hbase-standalone:1.0
    container_name: hbase
    ports:
      - 16000:16000
      - 16010:16010
      - 16020:16020
      - 16030:16030
      # - 2888:2888
      # - 3888:3888
      # - 2181:2181    

  
  pipeline-zookeeper-a:
    image: makengi12/zookeeper:1.0
    container_name: pipeline-zookeeper-a 
    restart: always 
    hostname: pipeline-zookeeper-a
    environment: 
        MY_ID: 1 
    volumes: 
      - pipeline-zookeeper-a-volume:/data

  pipeline-zookeeper-b: 
    image: makengi12/zookeeper:1.0
    container_name: pipeline-zookeeper-b 
    restart: always 
    hostname: pipeline-zookeeper-b
    environment: 
        MY_ID: 2 
    volumes: 
      - pipeline-zookeeper-b-volume:/data 
    
  pipeline-zookeeper-c: 
    image: makengi12/zookeeper:1.0
    container_name: pipeline-zookeeper-c  
    restart: always 
    hostname: pipeline-zookeeper-c
    environment: 
        MY_ID: 3 
    volumes: 
      - pipeline-zookeeper-c-volume:/data

  pipeline-kafka-1: 
    image: makengi12/kafka:1.0
    container_name: pipeline-kafka-1 
    environment: 
      BROKER_ID: 1 
    hostname: pipeline-kafka-1  
    restart: always 
    volumes: 
      - pipeline-kafka-1-volume:/data 
        
  pipeline-kafka-2: 
    image: makengi12/kafka:1.0
    container_name: pipeline-kafka-2 
    environment: 
      BROKER_ID: 2 
    hostname: pipeline-kafka-2 
    restart: always 
    volumes: 
      - pipeline-kafka-2-volume:/data 
    
  pipeline-kafka-3:
    image: makengi12/kafka:1.0
    container_name: pipeline-kafka-3 
    environment: 
      BROKER_ID: 3 
    hostname: pipeline-kafka-3 
    restart: always
    volumes: 
      - pipeline-kafka-3-volume:/data

  kafka_ui: 
    image: obsidiandynamics/kafdrop
    container_name: kafdrop 
    restart: "always"
    ports: 
      - "9020:9000"
    environment: 
      KAFKA_BROKERCONNECT: "pipeline-kafka-1:9092,pipeline-kafka-2:9092,pipeline-kafka-3:9092"
      JVM_OPTS: "-Xms32M -Xmx64M"

  flume:
    image: makengi12/flume:1.0
    container_name: flume
    restart: always
    ports:
      - "4045:4045" # Avro 


volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  airflow:
  zeppelin_logs:
  zeppelin_data:
  zeppelin_notebook:
  grafana:
  pipeline-zookeeper-a-volume:
  pipeline-zookeeper-b-volume:   
  pipeline-zookeeper-c-volume:
  pipeline-kafka-1-volume:
  pipeline-kafka-2-volume:
  pipeline-kafka-3-volume:
  
